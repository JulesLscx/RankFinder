{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tetex\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tetex\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = '../data/train_sample_theo.csv'\n",
    "test_file_path = '../data/test_sample_theo.csv'\n",
    "\n",
    "# Chargement des données\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tetex\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "X_train = list(train_data[\"revue/texte\"])\n",
    "y_train = np.array([int(i) - 1 for i in train_data[\"revue/score\"]])\n",
    "\n",
    "X_test = list(test_data[\"revue/texte\"])\n",
    "y_test = np.array([int(i) - 1 for i in test_data[\"revue/score\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(inputs):\n",
    "    return tokenizer(inputs, padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\n",
    "\n",
    "X_train_tokenized = tokenize(X_train)\n",
    "X_test_tokenized = tokenize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir BatchEncoding en un type hashable\n",
    "X_train_tokenized_hashable = (X_train_tokenized['input_ids'], X_train_tokenized['token_type_ids'], X_train_tokenized['attention_mask'])\n",
    "X_test_tokenized_hashable = (X_test_tokenized['input_ids'], X_test_tokenized['token_type_ids'], X_test_tokenized['attention_mask'])\n",
    "\n",
    "# Convertir les étiquettes en ensembles de données TensorFlow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tokenized_hashable, y_train)).shuffle(len(X_train)).batch(8)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_tokenized_hashable, y_test)).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation du modèle\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tetex\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tetex\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "624/624 [==============================] - 6209s 10s/step - loss: 1.4134 - accuracy: 0.4866 - val_loss: 1.3963 - val_accuracy: 0.4960\n",
      "Epoch 2/2\n",
      "624/624 [==============================] - 6290s 10s/step - loss: 1.4105 - accuracy: 0.4878 - val_loss: 1.3841 - val_accuracy: 0.4960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x260f3aa8670>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "model.fit(train_dataset, epochs=2, validation_data=test_dataset, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/polarity\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/polarity\\assets\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde du modèle\n",
    "save_path = '../models/polarity'\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 346s 3s/step - loss: 1.3841 - accuracy: 0.4960\n",
      "Eval Loss: 1.3840978145599365, Eval Accuracy: 0.495991975069046\n"
     ]
    }
   ],
   "source": [
    "# Évaluation du modèle\n",
    "eval_loss, eval_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Eval Loss: {eval_loss}, Eval Accuracy: {eval_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Classe prédite: 5\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Chemin vers le modèle sauvegardé\n",
    "model_path = '../models/polarity'\n",
    "\n",
    "# Chargement du modèle\n",
    "loaded_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Initialisation du tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def prepare_input(text):\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\n",
    "    return {'input_ids': tokens['input_ids'], 'token_type_ids': tokens['token_type_ids'], 'attention_mask': tokens['attention_mask']}\n",
    "\n",
    "# Exemple de texte à classer\n",
    "#text = \"Ce livre est vraiment génial, je le recommande à tout le monde !\"\n",
    "#text = \"Je déconseille ce livre, il est vraiment nul\"\n",
    "text = \"null\"\n",
    "\n",
    "# Préparation de l'entrée\n",
    "prepared_input = prepare_input(text)\n",
    "\n",
    "# Inférence\n",
    "predictions = loaded_model.predict(prepared_input)\n",
    "predicted_class = tf.argmax(predictions['logits'], axis=1).numpy()[0] + 1\n",
    "\n",
    "print(f\"Classe prédite: {predicted_class}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
